{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkGxEBNhZNWt"
   },
   "source": [
    "# Data Loading\n",
    "\n",
    "This notebook will document the functions and other code related with the loading of the datasets into Jupyter or Colab, from different sources like AWS-S3, google Drive  or a local repository, And in different formats (CSV, HDF5, feather, pickles) or with different techniques (batches, Dask, etc.)\n",
    "\n",
    "This is the first stage for all the subsequent stages of working with data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UHL4TC9fYZLv"
   },
   "outputs": [],
   "source": [
    "# First import the required libraries.\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K2hlrBAC2mbH"
   },
   "outputs": [],
   "source": [
    "# To load a file into colab:\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BL-G2VF-JU-M"
   },
   "outputs": [],
   "source": [
    "# To load a file from AWS-S3\n",
    "import boto3\n",
    "import logging\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnaGvjAoK5YJ"
   },
   "source": [
    "# Step 1: Find the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xD-T9zQaxhH5",
    "outputId": "54639f6d-553f-4cd5-8697-4a8bc8e314a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/PROJECT/Data\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Get the current directory\n",
    "print(os.getcwd())\n",
    "# Check if the directory exists\n",
    "print(os.path.exists('../Inferencia y recomendacion - EGM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "mfGNBUuGxhIG",
    "outputId": "5c488e18-3423-46aa-aa54-82484d273c6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/PROJECT/Inferencia y recomendacion - EGM\n"
     ]
    }
   ],
   "source": [
    "cd ../Inferencia y recomendacion - EGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "o5lvS6qQxhIR",
    "outputId": "03548396-3175-4b3f-ec24-6153512c968c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;32m8aca180564c2-Anexo_1___Segmentación_de_clientes_e_Inferencias_de_información__1_.pdf\u001b[0m\u001b[K*\r\n",
      "\u001b[01;32m'Acuerdo - Correlation One y EGM.pdf'\u001b[0m*\r\n",
      "\u001b[01;32m'Documento t'$'\\302\\202''cnico - MinTIC - RetoInferenciaRecomendacion.docx'\u001b[0m*\r\n",
      " \u001b[01;32mplacetopayDB1.csv\u001b[0m*\r\n",
      " \u001b[01;32mplacetopayDB2.csv\u001b[0m*\r\n",
      "\u001b[01;32m'Sebastian Londono - Inferencia y recomendacion - EGM.zip'\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "zcy1XbfsxhIn"
   },
   "outputs": [],
   "source": [
    "# This are the adresses where to find the file. You have to update this info:\n",
    "FileName = 'placetopayDB2.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDrj-mkcMJgw"
   },
   "source": [
    "## Upload Files to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MEt5lq_Uyay"
   },
   "outputs": [],
   "source": [
    "# To Upload files to Google Colab\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dom5XlC4Mu4_"
   },
   "outputs": [],
   "source": [
    "# Address of the file in Google Drive (This is only to show the file in drive, for reference):\n",
    "# DB1\n",
    "#https://drive.google.com/file/d/1IJWjFNE2Fz4o0TCdRmB-UijFtMounted at /content/gdrivei7m0MLH/view?usp=sharing\n",
    "\n",
    "# DB2\n",
    "URL = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "\n",
    "# Actual address to use in order to load the file into Colab:\n",
    "#FilePath = '/content/drive/My Drive/DS4A-3/Place to pay - DS4A - Databases and Notebooks/placetopayDB2.csv'\n",
    "\n",
    "#Merchants:\n",
    "FilePath = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qLuYwx0xhJB"
   },
   "source": [
    "# Step 2: Loading\n",
    "## Load CSV into colab from Google Drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "qfCn527tUnuN",
    "outputId": "a75b74ea-be64-4cea-a09b-13276f9fb681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# For Using Google Drive, (Only if executing notebook from Google colab):\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# after that:\n",
    "# <--- Refresh mounted Drive\n",
    "# <--- Look for file and get the path link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeZhRqbAxhJV"
   },
   "outputs": [],
   "source": [
    "# Load full database into Colab:\n",
    "bd = pd.read_csv( FilePath, encoding = 'utf-8')\n",
    "# Dataset is now stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAfuPhYjxhH2"
   },
   "source": [
    "## Load CSV into jupyter from local folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "WBkm-8pWxhId",
    "outputId": "b22dec83-4efd-4e34-87c8-c9309b3fe10a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_csv.reader at 0x7f0ce8dc4cf8>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To load full file. \n",
    "from csv import reader\n",
    "\n",
    "# if it fails it might be necessary to add encoding = \"utf-8\"\n",
    "opened_file = open('placetopayDB2.csv')   # File path \n",
    "read_file = reader(opened_file,delimiter=',')\n",
    "read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "f97ml8_BxhIv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is now stored in a Pandas Dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_user_agent</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>transaction_description</th>\n",
       "      <th>transaction_processing_date_</th>\n",
       "      <th>transaction_processing_hour</th>\n",
       "      <th>transaction_request_language</th>\n",
       "      <th>transaction_payer_id</th>\n",
       "      <th>transaction_payer_document_type</th>\n",
       "      <th>transaction_payer_email</th>\n",
       "      <th>IP</th>\n",
       "      <th>...</th>\n",
       "      <th>reason_code_iso</th>\n",
       "      <th>reason_description</th>\n",
       "      <th>reason_clasiffication</th>\n",
       "      <th>paymentmethod_name</th>\n",
       "      <th>paymentmethod_franchise</th>\n",
       "      <th>paymentmethod_type</th>\n",
       "      <th>isic_division_id</th>\n",
       "      <th>isic_division_name</th>\n",
       "      <th>isic_section_id</th>\n",
       "      <th>isic_section_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mozilla/5.0 (Linux; Android 8.0.0; SM-G930F) A...</td>\n",
       "      <td>COA1494204936</td>\n",
       "      <td>Pago con QR</td>\n",
       "      <td>16/09/2020</td>\n",
       "      <td>9</td>\n",
       "      <td>ES</td>\n",
       "      <td>D.B&gt;\"GH7@2U';:]7L?Y\"KAX2KQ=3&gt;H$E&amp;%W&lt;=:[L+%4</td>\n",
       "      <td>CRCPF</td>\n",
       "      <td>O`&gt;CT'APF_+(_E&amp;_`=&amp;!#68:+%^=6R;L&lt;Z4L!=&gt;\"_LO</td>\n",
       "      <td>L?B]A+]``$6);JIP\"7&lt;P]$.HD[*2&gt;X^N#6UQ#J\\E0GX]$....</td>\n",
       "      <td>...</td>\n",
       "      <td>00</td>\n",
       "      <td>Aprobada</td>\n",
       "      <td>Red/Banco</td>\n",
       "      <td>Transerver Mastercard</td>\n",
       "      <td>MASTERCARD</td>\n",
       "      <td>CREDITCARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mozilla/5.0 (iPhone; CPU iPhone OS 13_6_1 like...</td>\n",
       "      <td>COA1493496261</td>\n",
       "      <td>Pago por QR</td>\n",
       "      <td>02/09/2020</td>\n",
       "      <td>18</td>\n",
       "      <td>ES</td>\n",
       "      <td>ET]'&amp;;[G9N0U55`QCCX&gt;\"'-!7&gt;,U#JUJ(IS/G6F92\"5</td>\n",
       "      <td>CRCPF</td>\n",
       "      <td>KS8!F9H!\"(&lt;P\\(N6Q%$K%9HD=5^SGUSK)EQ453U3\\C/</td>\n",
       "      <td>D\\A\"`H@+T#6AS%G6HT(M]$.HYGA::C6;#6EBZD(Q+`Q]$....</td>\n",
       "      <td>...</td>\n",
       "      <td>?2</td>\n",
       "      <td>Transaccion Declinada Por Politicas De Control...</td>\n",
       "      <td>Políticas de control de riesgos</td>\n",
       "      <td>Transerver Visa</td>\n",
       "      <td>VISA</td>\n",
       "      <td>CREDITCARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mozilla/5.0 (Linux; Android 8.0.0; SM-G930F) A...</td>\n",
       "      <td>COA1493494554</td>\n",
       "      <td>Pago por QR</td>\n",
       "      <td>02/09/2020</td>\n",
       "      <td>18</td>\n",
       "      <td>ES</td>\n",
       "      <td>D.B&gt;\"GH7@2U';:]7L?Y\"KAX2KQ=3&gt;H$E&amp;%W&lt;=:[L+%4</td>\n",
       "      <td>CRCPF</td>\n",
       "      <td>O`&gt;CT'APF_+(_E&amp;_`=&amp;!#68:+%^=6R;L&lt;Z4L!=&gt;\"_LO</td>\n",
       "      <td>L?B]A+]``$6);JIP\"7&lt;P]$.HD[*2&gt;X^N#6UQ#J\\E0GX]$....</td>\n",
       "      <td>...</td>\n",
       "      <td>00</td>\n",
       "      <td>Aprobada</td>\n",
       "      <td>Red/Banco</td>\n",
       "      <td>Transerver Mastercard</td>\n",
       "      <td>MASTERCARD</td>\n",
       "      <td>CREDITCARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mozilla/5.0 (Linux; Android 10; SAMSUNG SM-G97...</td>\n",
       "      <td>COA1492916120</td>\n",
       "      <td>4395</td>\n",
       "      <td>25/08/2020</td>\n",
       "      <td>13</td>\n",
       "      <td>ES</td>\n",
       "      <td>FZ0#UZ`R')9'I8A^4$D$7_/\\*U=@G(CGY[#55S`[,!@</td>\n",
       "      <td>CRCPF</td>\n",
       "      <td>F_NL4'KE7W!AS+M[,M2FNKTSW$5DT)&lt;$8X-X\\RP^'W=</td>\n",
       "      <td>D3M%3I)LU#6Q.HC[C:6`]$.L!HM$@:CD#6I-:!AOV/L]$....</td>\n",
       "      <td>...</td>\n",
       "      <td>00</td>\n",
       "      <td>Aprobada</td>\n",
       "      <td>Red/Banco</td>\n",
       "      <td>Transerver Mastercard</td>\n",
       "      <td>MASTERCARD</td>\n",
       "      <td>CREDITCARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mozilla/5.0 (iPhone; CPU iPhone OS 13_6_1 like...</td>\n",
       "      <td>COA1493496172</td>\n",
       "      <td>Pago por QR</td>\n",
       "      <td>02/09/2020</td>\n",
       "      <td>18</td>\n",
       "      <td>ES</td>\n",
       "      <td>IUAE(,&gt;\"%&amp;@IPA-&gt;+UM58^?!+IM?E^6.T6Q,:OE&amp;PR9</td>\n",
       "      <td>CRCPF</td>\n",
       "      <td>OB:6$X[5R&amp;1Q24:K2;$&lt;Z[*-'J#4D&gt;8WW4I&lt;](_1P%#</td>\n",
       "      <td>L?B]A+]``$6);JIP\"7&lt;P]$.HD[*2&gt;X^N#6UQ#J\\E0GX]$....</td>\n",
       "      <td>...</td>\n",
       "      <td>?-</td>\n",
       "      <td>Transaccion Pendiente. Por Favor Consulte Con ...</td>\n",
       "      <td>Red/Banco</td>\n",
       "      <td>Transerver Visa</td>\n",
       "      <td>VISA</td>\n",
       "      <td>CREDITCARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              transaction_user_agent transaction_id  \\\n",
       "0  Mozilla/5.0 (Linux; Android 8.0.0; SM-G930F) A...  COA1494204936   \n",
       "1  Mozilla/5.0 (iPhone; CPU iPhone OS 13_6_1 like...  COA1493496261   \n",
       "2  Mozilla/5.0 (Linux; Android 8.0.0; SM-G930F) A...  COA1493494554   \n",
       "3  Mozilla/5.0 (Linux; Android 10; SAMSUNG SM-G97...  COA1492916120   \n",
       "4  Mozilla/5.0 (iPhone; CPU iPhone OS 13_6_1 like...  COA1493496172   \n",
       "\n",
       "  transaction_description transaction_processing_date_  \\\n",
       "0             Pago con QR                   16/09/2020   \n",
       "1             Pago por QR                   02/09/2020   \n",
       "2             Pago por QR                   02/09/2020   \n",
       "3                    4395                   25/08/2020   \n",
       "4             Pago por QR                   02/09/2020   \n",
       "\n",
       "   transaction_processing_hour transaction_request_language  \\\n",
       "0                            9                           ES   \n",
       "1                           18                           ES   \n",
       "2                           18                           ES   \n",
       "3                           13                           ES   \n",
       "4                           18                           ES   \n",
       "\n",
       "                          transaction_payer_id  \\\n",
       "0  D.B>\"GH7@2U';:]7L?Y\"KAX2KQ=3>H$E&%W<=:[L+%4   \n",
       "1  ET]'&;[G9N0U55`QCCX>\"'-!7>,U#JUJ(IS/G6F92\"5   \n",
       "2  D.B>\"GH7@2U';:]7L?Y\"KAX2KQ=3>H$E&%W<=:[L+%4   \n",
       "3  FZ0#UZ`R')9'I8A^4$D$7_/\\*U=@G(CGY[#55S`[,!@   \n",
       "4  IUAE(,>\"%&@IPA->+UM58^?!+IM?E^6.T6Q,:OE&PR9   \n",
       "\n",
       "  transaction_payer_document_type  \\\n",
       "0                           CRCPF   \n",
       "1                           CRCPF   \n",
       "2                           CRCPF   \n",
       "3                           CRCPF   \n",
       "4                           CRCPF   \n",
       "\n",
       "                       transaction_payer_email  \\\n",
       "0  O`>CT'APF_+(_E&_`=&!#68:+%^=6R;L<Z4L!=>\"_LO   \n",
       "1  KS8!F9H!\"(<P\\(N6Q%$K%9HD=5^SGUSK)EQ453U3\\C/   \n",
       "2  O`>CT'APF_+(_E&_`=&!#68:+%^=6R;L<Z4L!=>\"_LO   \n",
       "3  F_NL4'KE7W!AS+M[,M2FNKTSW$5DT)<$8X-X\\RP^'W=   \n",
       "4  OB:6$X[5R&1Q24:K2;$<Z[*-'J#4D>8WW4I<](_1P%#   \n",
       "\n",
       "                                                  IP  ... reason_code_iso  \\\n",
       "0  L?B]A+]``$6);JIP\"7<P]$.HD[*2>X^N#6UQ#J\\E0GX]$....  ...              00   \n",
       "1  D\\A\"`H@+T#6AS%G6HT(M]$.HYGA::C6;#6EBZD(Q+`Q]$....  ...              ?2   \n",
       "2  L?B]A+]``$6);JIP\"7<P]$.HD[*2>X^N#6UQ#J\\E0GX]$....  ...              00   \n",
       "3  D3M%3I)LU#6Q.HC[C:6`]$.L!HM$@:CD#6I-:!AOV/L]$....  ...              00   \n",
       "4  L?B]A+]``$6);JIP\"7<P]$.HD[*2>X^N#6UQ#J\\E0GX]$....  ...              ?-   \n",
       "\n",
       "                                  reason_description  \\\n",
       "0                                           Aprobada   \n",
       "1  Transaccion Declinada Por Politicas De Control...   \n",
       "2                                           Aprobada   \n",
       "3                                           Aprobada   \n",
       "4  Transaccion Pendiente. Por Favor Consulte Con ...   \n",
       "\n",
       "             reason_clasiffication     paymentmethod_name  \\\n",
       "0                        Red/Banco  Transerver Mastercard   \n",
       "1  Políticas de control de riesgos        Transerver Visa   \n",
       "2                        Red/Banco  Transerver Mastercard   \n",
       "3                        Red/Banco  Transerver Mastercard   \n",
       "4                        Red/Banco        Transerver Visa   \n",
       "\n",
       "  paymentmethod_franchise  paymentmethod_type isic_division_id  \\\n",
       "0              MASTERCARD          CREDITCARD              NaN   \n",
       "1                    VISA          CREDITCARD              NaN   \n",
       "2              MASTERCARD          CREDITCARD              NaN   \n",
       "3              MASTERCARD          CREDITCARD              NaN   \n",
       "4                    VISA          CREDITCARD              NaN   \n",
       "\n",
       "  isic_division_name  isic_section_id isic_section_name  \n",
       "0                NaN              NaN               NaN  \n",
       "1                NaN              NaN               NaN  \n",
       "2                NaN              NaN               NaN  \n",
       "3                NaN              NaN               NaN  \n",
       "4                NaN              NaN               NaN  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to check schema is as expected: \n",
    "Sample = pd.read_csv(FileName, nrows = 10) \n",
    "# Dataset is now stored in a Pandas Dataframe, but only the first 10 rows\n",
    "# maybe you need to use sep=',', error_bad_lines=False, encoding='utf-8' / 'latin1'\n",
    "print('Dataset is now stored in a Pandas Dataframe')\n",
    "Sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAw6LkjIQ5cW"
   },
   "outputs": [],
   "source": [
    "# Load the database to start exploratory analysis:\n",
    "bd = pd.read_csv( FileName, encoding = 'utf-8')\n",
    "# Dataset is now stored in a Pandas Dataframe\n",
    "bd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RoGxMZRUVG5"
   },
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hY62p6flUUDl"
   },
   "outputs": [],
   "source": [
    "# Load the first rows of the database to start exploratory analysis:\n",
    "bd = pd.read_csv( FileName, encoding = 'utf-8', nrows = 100000) # Load a Sample of the first 100 000 rows\n",
    "# Dataset is now stored in a Pandas Dataframe\n",
    "bd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Count the lines\n",
    "num_lines = sum(1 for l in open(FileName))\n",
    "\n",
    "# Sample size - in this case: 1000 000 rows\n",
    "Rows = 1000000\n",
    "\n",
    "# The row indices to skip - make sure 0 is not included to keep the header!\n",
    "skip_idx = random.sample(range(1, num_lines), num_lines - Rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read a sample of the data, in batches\n",
    "DataChunk = pd.read_csv(FileName, skiprows=skip_idx, chunksize=100000, sep=',', encoding='utf-8') #latin1 didnt work for accents\n",
    "dfList = []\n",
    "for chunk in DataChunk:\n",
    "    dfList.append(pd.DataFrame(chunk))\n",
    "    print(chunk.shape, type(chunk))\n",
    "    del chunk\n",
    "bd = pd.concat(dfList,sort=False)\n",
    "del DataChunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvLiw0YGX1JG"
   },
   "outputs": [],
   "source": [
    "# ---------- To load files in batches (chunks) into Jupyter:\n",
    "#change this      movies_tv = pd.read_json(\"movies_crop.csv\", lines=True)\n",
    "#for this\n",
    "iterator = pd.read_csv('movies_crop.csv', chunksize = 100000)\n",
    "movies_tv = pd.concat(iterator)\n",
    "del iterator\n",
    "\n",
    "#If you’re using the compressed csv files the solution is similar,\n",
    "iterator = pd.read_json('reviews_Movies_and_TV_5.json.gz', lines = True, compression = 'gzip', chunksize = 100000)\n",
    "movies_tv = pd.concat(iterator)\n",
    "del iterator\n",
    "\n",
    "# This is the size of the batch. The number depends on the system you are working with, and the shape of the file. \n",
    "ChunkSize = 10 ** 5 \n",
    "\n",
    "#https://pythonspeed.com/articles/chunking-pandas/\n",
    "\n",
    "# structure program as:\n",
    "#for chunk in pd.read_csv(FileName, chunksize=ChunkSize):\n",
    "#    process(chunk)\n",
    "\n",
    "#That should load the entire dataset and delete the iterator construct to free the RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "zBJup74BU7Ql"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (37) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(100000, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "(25791, 47) <class 'pandas.core.frame.DataFrame'>\n",
      "CPU times: user 3min 51s, sys: 44.2 s, total: 4min 35s\n",
      "Wall time: 6min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DataChunk = pd.read_csv(FileName, chunksize=100000, sep=',', encoding='latin1') #utf-8\n",
    "dfList = []\n",
    "for chunk in DataChunk:\n",
    "    dfList.append(pd.DataFrame(chunk))\n",
    "    print(chunk.shape, type(chunk))\n",
    "    del chunk                         # You have to liberate memory, otherwise, it will crash the kernel\n",
    "bd = pd.concat(dfList,sort=False)     # You can also convert to dataframe and process inside the for loop.\n",
    "del DataChunk                         # You have to liberate memory, otherwise, it will crash the kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RM1Ndq4zW6wP"
   },
   "outputs": [],
   "source": [
    "# NOT WORK\n",
    "\n",
    "bd = pd.concat((chunk for chunk in pd.read_csv(FileName, dtype='str', sep=',', encoding='latin1', chunksize=5000))) #error_bad_lines=False\n",
    "# Dataset is now stored in a Pandas Dataframe\n",
    "\n",
    "# ERROR ERROR: /usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1,11,12,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
    "#  interactivity=interactivity, compiler=compiler, result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGcN0615TyZ2"
   },
   "outputs": [],
   "source": [
    "# NOT WORK\n",
    "# Load in batches into Colab:\n",
    "bd = pd.read_csv( FilePath, encoding = 'utf-8', nrows = ChunkSize )  # nrows is a sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQMsRIp2UpEq"
   },
   "source": [
    "## To Load CSV from AWS S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVlfLVdXUzQa"
   },
   "outputs": [],
   "source": [
    "# Access key to the AWS Server:\n",
    "AWS_KEY_ID=\"XXXXXXXXXXXXXXX\"\n",
    "AWS_SECRET=\"XXXXXXXXXXXXXXX\"\n",
    "bucket = \"XXXXXXXXXXXXXXX\"\n",
    "key= \"XXXXXXXXXXXXXXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df9ZiwsRU1gJ"
   },
   "outputs": [],
   "source": [
    "# Configure Boto3 to interfase with AWS server:\n",
    "s3 = boto3.client(\"s3\", \n",
    "                  region_name='us-east-1', \n",
    "                  aws_access_key_id=AWS_KEY_ID, \n",
    "                  aws_secret_access_key=AWS_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mmfmxZINU3QF"
   },
   "outputs": [],
   "source": [
    "# Read Full file from S3 Bucket:\n",
    "read_file = s3.get_object(Bucket=bucket,Key=key)\n",
    "# print(read_file['Body'])\n",
    "rows = 1000000\n",
    "bd = pd.read_csv(read_file['Body'], nrows=rows)  #nrows is a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obAqWIONB5qx"
   },
   "source": [
    "## Working with compressed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqDKBxQCB8dI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Save a single file:\n",
    "jungle_zip = zipfile.ZipFile('C:\\\\Stories\\\\Fantasy\\\\jungle.zip', 'w')\n",
    "jungle_zip.write('C:\\\\Stories\\\\Fantasy\\\\jungle.pdf', compress_type=zipfile.ZIP_DEFLATED)\n",
    "jungle_zip.close()\n",
    "\n",
    "# Extract a single file:         \n",
    "fantasy_zip = zipfile.ZipFile('C:\\\\Stories\\\\Fantasy\\\\archive.zip')\n",
    "fantasy_zip.extract('Fantasy Jungle.pdf', 'C:\\\\Stories\\\\Fantasy')\n",
    "fantasy_zip.close()\n",
    "\n",
    "with ZipFile('foo.zip', 'r') as zf:\n",
    "    zf.extractall('destination_path/')\n",
    "\n",
    "# Extract All:\n",
    "fantasy_zip = zipfile.ZipFile('C:\\\\Stories\\\\Fantasy\\\\archive.zip')\n",
    "fantasy_zip.extractall('C:\\\\Library\\\\Stories\\\\Fantasy')\n",
    "jungle_zip.close()\n",
    "\n",
    "# Write all PDFs from a folder \n",
    "fantasy_zip = zipfile.ZipFile('C:\\\\Stories\\\\Fantasy\\\\archive.zip', 'w')\n",
    "for folder, subfolders, files in os.walk('C:\\\\Stories\\\\Fantasy'):\n",
    "    for file in files:\n",
    "        if file.endswith('.pdf'):\n",
    "            fantasy_zip.write(os.path.join(folder, file), os.path.relpath(os.path.join(folder,file), 'C:\\\\Stories\\\\Fantasy'), compress_type = zipfile.ZIP_DEFLATED)\n",
    " \n",
    "fantasy_zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yw5E_7e6MajX"
   },
   "outputs": [],
   "source": [
    "# Read compressed json files (with Gz)\n",
    "instant_video = pd.read_json(\"reviews_Amazon_Instant_Video.json.gz\", lines=True, compression='gzip')\n",
    "print('instant_video loaded')\n",
    "\n",
    "Using uncompressed, but cropped files:\n",
    "instant_video = pd.read_csv('instvideo_crop.csv', infer_datetime_format = True, parse_dates = True)\n",
    "instant_video['datetime'] = pd.to_datetime(instant_video['datetime'], format=\"%Y-%m-%d\")\n",
    "instant_video['helpful'] = pd.DataFrame(instant_video['helpful'].str.replace(pat = \"['\\[\\] ]\", repl = '').str.split(pat = ',', expand = False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTLX2f15H0QB"
   },
   "source": [
    "## Using Feather Format\n",
    "Feather format is designed to interoperability between Python and R. Its reading and writing speeds are much more improved when working with Pandas categorical columns. \n",
    "\n",
    "Using feather enables faster I/O speeds and less memory. However, since it is an evolving format it is recommended to use it for quick loading and transformation related data processing rather than using it as a long term storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feather-format\n",
      "  Downloading https://files.pythonhosted.org/packages/67/e8/ee99f142f19d35588501943510f8217f9dd77184574b0c933c53218e0f19/feather-format-0.4.1.tar.gz\n",
      "Collecting pyarrow>=0.4.0 (from feather-format)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/9b/887d1d03d3d43706dee3a71cdad9f9bbb8fe74fc93d8db5d663f5bf34e48/pyarrow-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (16.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 16.6MB 1.4MB/s ta 0:00:011    78% |█████████████████████████       | 13.0MB 2.5MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.6/site-packages (from pyarrow>=0.4.0->feather-format) (1.19.2)\n",
      "Building wheels for collected packages: feather-format\n",
      "  Running setup.py bdist_wheel for feather-format ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/0b/c4/c6/2b568ff878182aab814bcfd3db31fdd0055a4b2a249a7921eb\n",
      "Successfully built feather-format\n",
      "Installing collected packages: pyarrow, feather-format\n",
      "Successfully installed feather-format-0.4.1 pyarrow-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install feather-format\n",
    "#!pip install feather-format\n",
    "import feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YwHtx1fOH2-L"
   },
   "outputs": [],
   "source": [
    "FilePath = \"./filename.ftr\" # or 'my_data.feather'\n",
    "\n",
    "# Write: Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file\n",
    "dataFrame.to_feather(FilePath) # You can put additional keywords as compression, compression_level, chunksize\n",
    "\n",
    "# Read: columns specify a sequence of columns to read. use_Threads is to parallelize\n",
    "readFrame = pd.read_feather(FilePath, columns=None, use_threads=True)\n",
    "print(readFrame)\n",
    "\n",
    "#Using only pandas:\n",
    "# Write alternative\n",
    "feather.write_dataframe(df, FilePath)\n",
    "# Read Alternative:\n",
    "df = feather.read_dataframe(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feather\n",
    "FilePath = \"./placetopayDB2.ftr\"\n",
    "#feather.write_dataframe(bd, FilePath)\n",
    "bd.to_feather(FilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read feather-format:\n",
    "bd = pd.read_feather(\"./placetopayDB4.ftr\", columns=None, use_threads=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AT5a_eXbVVC6"
   },
   "source": [
    "## Using Pickles\n",
    "\n",
    "Pickle files are a type of file (and a package included in pandas) that allows to store many kinds of objects (but not all) in hard disk to be read again after.\n",
    "\n",
    "Using this technique is possible to run parts of your code in different scripts or different Jupyter notebooks or even different machines and integrate again after they run. More details:\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_pickle.html https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_pickle.html https://docs.python.org/3/library/pickle.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVHJ1ecCVmF_"
   },
   "outputs": [],
   "source": [
    "# To use that you can do this:\n",
    "df.to_pickle('path_filename')\n",
    "\n",
    "#And then when you need to restart your notebook you can simply read directly from pickle:\n",
    "df = pd.read_pickle('path_filename')\n",
    "\n",
    "# If the file is very large the pickle file will be compressed with the parameter compression = 'gzip'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd.to_pickle('./placetopayDB2_pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Q519HxaOMRk"
   },
   "source": [
    "## How to DASK "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "0. DataBase_Loading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}